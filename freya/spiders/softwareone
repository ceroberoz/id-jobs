import scrapy
import json
from datetime import datetime
import logging
from typing import Dict, Any, Optional
import random

logger = logging.getLogger(__name__)

class SoftwareOneSpiderJson(scrapy.Spider):
    name = 'softwareone'
    BASE_URL = 'https://careers.softwareone.com'
    API_URL = f'{BASE_URL}/api/jobs?country=Indonesia&page=1&sortBy=posted_date&descending=true&internal=false&deviceId=undefined&domain=softwareone.jibeapply.com'

    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:90.0) Gecko/20100101 Firefox/90.0',
    ]

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.timestamp = datetime.now().strftime("%d/%m/%Y %H:%M:%S")

    def start_requests(self):
        headers = {
            'accept': 'application/json, text/plain, */*',
            'accept-language': 'en-US,en;q=0.9',
            'cache-control': 'no-cache',
            'dnt': '1',
            'pragma': 'no-cache',
            'priority': 'u=1, i',
            'referer': 'https://careers.softwareone.com/en/jobs?country=Indonesia&page=1',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'sec-gpc': '1',
            'user-agent': random.choice(self.USER_AGENTS)
        }
        yield scrapy.Request(self.API_URL, headers=headers, callback=self.parse)

    def parse(self, response):
        try:
            data = json.loads(response.text)
            jobs = data.get('jobs', [])

            if not jobs:
                logger.info("No jobs found for the given criteria.")
                return

            for job in jobs:
                yield self.parse_job(job)

        except json.JSONDecodeError as e:
            logger.error(f"Error decoding JSON: {e}")
            logger.debug(f"Response content: {response.text}")
        except Exception as e:
            logger.error(f"Unexpected error: {e}")

    def parse_job(self, job: Dict[str, Any]) -> Dict[str, Any]:
        return {
            'job_title': self.sanitize_string(job.get('title')),
            'job_location': self.sanitize_string(job.get('location', {}).get('city')),
            'job_department': self.sanitize_string(job.get('department')),
            'job_url': f"{self.BASE_URL}/en/job/{job.get('id')}",
            'first_seen': self.timestamp,
            'base_salary': 'N/A',
            'job_type': self.sanitize_string(job.get('type')),
            'job_level': self.sanitize_string(job.get('experienceLevel')),
            'job_apply_end_date': 'N/A',
            'last_seen': self.format_unix_time(job.get('postedDate')),
            'is_active': 'True',
            'company': 'SoftwareOne',
            'company_url': self.BASE_URL,
            'job_board': 'SoftwareOne Careers',
            'job_board_url': f"{self.BASE_URL}/en/jobs"
        }

    @staticmethod
    def sanitize_string(s: Optional[str]) -> str:
        return s.strip() if s else 'N/A'

    @staticmethod
    def format_unix_time(unix_time: Optional[int]) -> str:
        if unix_time is None:
            return 'N/A'
        try:
            return datetime.fromtimestamp(unix_time / 1000).strftime('%Y-%m-%d %H:%M:%S')
        except (ValueError, TypeError):
            return 'N/A'